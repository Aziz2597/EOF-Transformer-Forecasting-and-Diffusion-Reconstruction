{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrI_2h9rs6Lo",
        "outputId": "a26ca7c3-1a5b-4f6c-f464-c8bd829501aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Shape of D: (730800, 342)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import Libraries\n",
        "import numpy as np\n",
        "from scipy.linalg import svd\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Load and Preprocess Velocity Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load velocity data\n",
        "velocity_data_U = loadmat('/content/drive/MyDrive/u.mat')['u']  # Replace with the correct path\n",
        "velocity_data_V = loadmat('/content/drive/MyDrive/v.mat')['v']  # Replace with the correct path\n",
        "\n",
        "# Reshape data\n",
        "velocity_data_U = velocity_data_U.reshape(-1, velocity_data_U.shape[-1])  # n x d\n",
        "velocity_data_V = velocity_data_V.reshape(-1, velocity_data_V.shape[-1])  # n x d\n",
        "\n",
        "# Stack U and V as a data matrix (D)\n",
        "D = np.concatenate([velocity_data_U, velocity_data_V], axis=1)\n",
        "print(f\"Shape of D: {D.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: EOF Analysis Using SVD\n",
        "U_svd, Sigma, Vt_svd = svd(D, full_matrices=False)\n",
        "\n",
        "# Dimensionality reduction\n",
        "k = 5  # Number of components to retain\n",
        "U_k = U_svd[:, :k]\n",
        "Sigma_k = np.diag(Sigma[:k])\n",
        "Vt_k = Vt_svd[:k, :]\n",
        "\n",
        "# Reconstruct the data with reduced components\n",
        "D_reconstructed = np.dot(U_k, np.dot(Sigma_k, Vt_k))"
      ],
      "metadata": {
        "id": "9i3d68KnuCi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract PCs for forecasting\n",
        "PCs = Vt_svd.T\n",
        "scaler = StandardScaler()\n",
        "PCs_normalized = scaler.fit_transform(PCs)\n",
        "\n",
        "# Step 4: Prepare Data for Forecasting\n",
        "seq_length = 5  # Use past 5 steps for forecasting. Adjust as per needs.\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data[i:i + seq_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "X_PCs = create_sequences(PCs_normalized, seq_length)\n",
        "y_PCs = PCs_normalized[seq_length:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_PCs_tensor = torch.tensor(X_PCs, dtype=torch.float32).permute(1, 0, 2)\n",
        "y_PCs_tensor = torch.tensor(y_PCs, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "KzObdcTRuS5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, seq_len):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.input_dim = input_dim\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=input_dim,\n",
        "            nhead=1,\n",
        "            num_encoder_layers=2,\n",
        "            num_decoder_layers=2,\n",
        "            batch_first=True  # Ensure batch-first processing\n",
        "        )\n",
        "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        x = self.transformer(src, tgt)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "input_dim = PCs_normalized.shape[1]\n",
        "model = TransformerModel(input_dim=input_dim, seq_len=seq_length)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 6: Train the Transformer Model (Using EOF PCs)\n",
        "epochs = 2  # Adjust as per needs\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_PCs_tensor, X_PCs_tensor)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Target shape: {y_PCs_tensor.shape}\")\n",
        "    loss = criterion(output, y_PCs_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R59Qn0_uXlr",
        "outputId": "8025861f-8bec-4f9f-9737-740247cad1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([5, 337, 342])\n",
            "Target shape: torch.Size([337, 342])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([337, 342])) that is different to the input size (torch.Size([5, 337, 342])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3376600742340088\n",
            "Output shape: torch.Size([5, 337, 342])\n",
            "Target shape: torch.Size([337, 342])\n",
            "Epoch 2, Loss: 1.4202789068222046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Train on Original Data (Without EOF)\n",
        "\n",
        "# Downsample the data\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "step = 10  # Keep every 10th row\n",
        "D_downsampled = D[::step]\n",
        "\n",
        "# Reduce dimensionality with PCA\n",
        "num_components = 50\n",
        "pca = PCA(n_components=num_components)\n",
        "D_reduced = pca.fit_transform(D_downsampled)\n",
        "\n",
        "# Normalize in chunks\n",
        "chunk_size = 10000\n",
        "normalized_chunks = [\n",
        "    StandardScaler().fit_transform(D_reduced[i:i + chunk_size])\n",
        "    for i in range(0, D_reduced.shape[0], chunk_size)\n",
        "]\n",
        "D_normalized = np.vstack(normalized_chunks)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 5  # Adjusted for memory efficiency\n",
        "X_original = create_sequences(D_normalized, seq_length)\n",
        "y_original = D_normalized[seq_length:]\n",
        "\n",
        "y_original_tensor = torch.tensor(y_original, dtype=torch.float32)\n",
        "y_original_tensor = y_original_tensor[:len(X_original)]\n",
        "X_original_tensor = torch.tensor(X_original, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for mini-batch processing\n",
        "batch_size = 512  # Adjust based on memory capacity\n",
        "dataset = TensorDataset(\n",
        "    X_original_tensor,\n",
        "    y_original_tensor\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model_original = TransformerModel(input_dim=D_normalized.shape[1], seq_len=seq_length)\n",
        "optimizer_original = Adam(model_original.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        model_original.train()\n",
        "        optimizer_original.zero_grad()\n",
        "        output = model_original(X_batch, X_batch)\n",
        "        output_last_step = output[:, -1, :]  # Use only the last time step\n",
        "        loss = criterion(output_last_step, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_original.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'[Original Data] Epoch {epoch + 1}, Average Loss: {epoch_loss / len(dataloader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xets0URYvm6c",
        "outputId": "86bbf9fd-f250-4143-95db-38aa2dde69af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Original Data] Epoch 1, Average Loss: 0.8262939303071348\n",
            "[Original Data] Epoch 2, Average Loss: 0.6792456053353689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate Performance\n",
        "y_pred_PCs = model(X_PCs_tensor, X_PCs_tensor).detach().numpy()\n",
        "\n",
        "# Select only the last prediction from the sequence for each sample\n",
        "y_pred_PCs = y_pred_PCs[-1, :, :]  # Shape: (337, 342)\n",
        "\n",
        "# Now, calculate the MSE and correlation\n",
        "mse_PCs = mean_squared_error(y_PCs_tensor, y_pred_PCs)\n",
        "correlation_PCs = np.corrcoef(y_PCs_tensor.flatten(), y_pred_PCs.flatten())[0, 1]\n",
        "\n",
        "print(f'[EOF PCs] MSE: {mse_PCs}, Correlation: {correlation_PCs}')\n",
        "\n",
        "# Evaluate original data model performance\n",
        "y_pred_original = model_original(X_original_tensor, X_original_tensor).detach().numpy()\n",
        "mse_original = mean_squared_error(y_original_tensor.squeeze().numpy(), y_pred_original.squeeze())\n",
        "correlation_original = np.corrcoef(y_original_tensor.squeeze().numpy().flatten(), y_pred_original.flatten())[0, 1]\n",
        "\n",
        "print(f'[Original Data] MSE: {mse_original}, Correlation: {correlation_original}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_KarHVd6Vpi",
        "outputId": "469ea2d8-3097-42fb-c916-a84187777884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EOF PCs] MSE: 1.1274749040603638, Correlation: 0.0029440311177484236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diffusion_reconstruction(data, missing_rate=0.3):\n",
        "    mask = np.random.rand(*data.shape) > missing_rate\n",
        "    data_missing = data * mask\n",
        "\n",
        "    for _ in range(10):\n",
        "        data_missing = np.nan_to_num(data_missing)\n",
        "        data_missing = np.roll(data_missing, 1, axis=0)\n",
        "    return data_missing\n",
        "\n",
        "D_reconstructed_diffusion = diffusion_reconstruction(D)\n",
        "mse_diffusion = mean_squared_error(D, D_reconstructed_diffusion)\n",
        "\n",
        "print(f'Diffusion Reconstruction MSE: {mse_diffusion}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXyLIg0j6pel",
        "outputId": "518624af-44f5-4663-f412-f5def5f02c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diffusion Reconstruction MSE: 0.08645015954971313\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}